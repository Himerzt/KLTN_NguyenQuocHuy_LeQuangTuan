# model_api.py
import asyncio
import base64
import time
import cv2
import numpy as np
import aiohttp
import uvicorn
from ultralytics import YOLO 
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input,  Dense, Dropout, LSTM)
import requests
from fastapi import File, UploadFile, Form, BackgroundTasks

model_yolo = YOLO("yolov8m.pt")

DEBUG = False

NUM_FRAMES = 12           # S·ªë frame li√™n ti·∫øp ƒë∆∞a v√†o LSTM m·ªói l·∫ßn d·ª± ƒëo√°n  
IMG_SIZE = 224            # K√≠ch th∆∞·ªõc ·∫£nh ƒë·∫ßu v√†o (224x224)  
NUM_JOINTS = 25           # S·ªë kh·ªõp (keypoints) m√† OpenPose tr√≠ch xu·∫•t
NUM_FEATURES = NUM_JOINTS * 3 # M·ªói kh·ªõp c√≥ (x, y, c) ‚Üí t·ªïng 75 gi√° tr·ªã  

ACTIONS = ["CLIMB", "FALL", "LIEDOWN", "SIT", "STAND"]

MODEL_WEIGHTS = r"kp_handle_miss.weights.h5"
OPENPOSE_URL = "http://127.0.0.1:8001/extract-keypoints"

PROCESS_FPS = 10           # S·ªë frame x·ª≠ l√Ω m·ªói gi√¢y (10 fps)  
PROCESS_INTERVAL = 1.0 / PROCESS_FPS  # Th·ªùi gian gi·ªØa 2 l·∫ßn x·ª≠ l√Ω (~0.1 gi√¢y)  
FRAME_QUEUE_MAXSIZE = 1    # H√†ng ƒë·ª£i frame t·ªëi ƒëa (tr√°nh tr·ªÖ x·ª≠ l√Ω)
NUM_CLASSES = len(ACTIONS)

TELEGRAM_BOT_TOKEN = "8464653213:AAHbmJ9sEUuhcIvUTY1vaMiFxIDkG2Wa5Z8"
TELEGRAM_CHAT_ID = "5855449751"

# ================== BUILD MODEL ==================
kp_input = Input(shape=(NUM_FRAMES, 75), name="keypoints")

xk = LSTM(128, return_sequences=True, name="kp_lstm1")(kp_input)
xk = Dropout(0.3, name="kp_dropout1")(xk)

xk = LSTM(128, name="kp_lstm2")(xk)
xk = Dropout(0.3, name="kp_dropout2")(xk)

xk = Dense(128, activation="relu", name="kp_dense")(xk)
xk = Dropout(0.3, name="kp_dropout3")(xk)

out = Dense(NUM_CLASSES, activation="softmax", name="cls_head")(xk)

model = Model(inputs=kp_input, outputs=out, name="kp_only_model")

model.load_weights(MODEL_WEIGHTS)
print("LSTM model loaded successfully")

# ================== APP INIT ==================
app = FastAPI(title="Action Recognition API (YOLO + OpenPose + LSTM)")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

frame_buffer = {}   # L∆∞u t·∫°m c√°c frame video theo t·ª´ng ng∆∞·ªùi (ID ri√™ng)  
kp_buffer = {}      # L∆∞u t·∫°m keypoints (t·ªça ƒë·ªô kh·ªõp) t∆∞∆°ng ·ª©ng m·ªói ng∆∞·ªùi 

aiohttp_session = None

person_history = {} 


@app.on_event("startup")
async def startup_event():
    global aiohttp_session
    aiohttp_session = aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=20, force_close=False))
    print("aiohttp session created")


@app.on_event("shutdown")
async def shutdown_event():
    global aiohttp_session
    if aiohttp_session:
        await aiohttp_session.close()
        print("aiohttp session closed")


async def call_openpose_async(cropped_frame):
    """Send cropped frame to OpenPose API and return keypoints."""
    global aiohttp_session
    try:
        _, jpg = cv2.imencode(".jpg", cropped_frame)
        form = aiohttp.FormData()
        form.add_field("file", jpg.tobytes(), filename="frame.jpg", content_type="image/jpeg")
        async with aiohttp_session.post(OPENPOSE_URL, data=form, timeout=10) as resp:
            res = await resp.json()
            return res.get("keypoints", [])
    except Exception as e:
        print("OpenPose call failed:", e)
        return []


async def do_model_predict(frames_list, kps_list):
    kp_np = np.array(kps_list)[np.newaxis, ...].astype(np.float32)
    print(f"[DEBUG] kp_np.shape={kp_np.shape}")

    try:
        preds = await asyncio.to_thread(model.predict, kp_np)
        idx = int(np.argmax(preds))
        return ACTIONS[idx], float(np.max(preds))
    except Exception as e:
        print("Predict error:", e)
        return "ERROR", 0.0


# ================== MAIN WEBSOCKET ==================
@app.websocket("/ws")
async def websocket_endpoint(ws: WebSocket):
    await ws.accept()
    print("üì° Client connected")

    frame_queue = asyncio.Queue(maxsize=FRAME_QUEUE_MAXSIZE)

    
        
    async def receiver():
        """Receive frames from frontend."""
        try:
            while True:
                data = await ws.receive_text()
                if not data:
                    continue
                try:
                    if frame_queue.full():
                        _ = frame_queue.get_nowait()
                except asyncio.QueueEmpty:
                    pass
                await frame_queue.put(data)
        except WebSocketDisconnect:
            return
        except Exception as e:
            print("Receiver error:", e)
            return

    
    # L∆∞u v·ªã tr√≠ bbox c≈© cho m·ªói ID ƒë·ªÉ l√†m m∆∞·ª£t di chuy·ªÉn
    miss_count = {}
    openpose_tasks = {}  # l∆∞u task async OpenPose cho m·ªói pid
    last_kps = {}        # l∆∞u keypoints cu·ªëi c√πng cho m·ªói pid (fallback)
    async def processor():
        """Process latest frame: YOLO ‚Üî OpenPose ‚Üî LSTM (multi-person, parallel)."""
        try:
            while True:
                try:
                    # L·∫•y frame m·ªõi nh·∫•t t·ª´ queue 
                    data = await asyncio.wait_for(frame_queue.get(), timeout=1.0)
                except asyncio.TimeoutError:
                    continue

                try:
                    if "," in data:
                        _, encoded = data.split(",", 1)
                    else:
                        encoded = data
                    img_bytes = base64.b64decode(encoded)
                    np_img = np.frombuffer(img_bytes, np.uint8)
                    if np_img.size == 0: continue
                    frame = cv2.imdecode(np_img, cv2.IMREAD_COLOR)
                    if frame is None: continue
                except Exception as e:
                    print("decode fail:", e)
                    continue


                t1 = time.time()

                # YOLO detection
                results = model_yolo.track(
                    source=frame,
                    persist=True,
                    stream=False,
                    verbose=False,
                    tracker="botsort.yaml"
                )

                tracked_persons = []
                if results and len(results) > 0:
                    boxes = results[0].boxes
                    if boxes is not None and boxes.data.numel() > 0:
                        for box in boxes:
                            cls = int(box.cls[0])
                            conf = float(box.conf[0])
                            if cls == 0 and conf > 0.15:  # ch·ªâ l·∫•y ng∆∞·ªùi
                                x1, y1, x2, y2 = map(int, box.xyxy[0])
                                track_id = int(box.id[0]) if box.id is not None else -1
                                tracked_persons.append({
                                    "id": track_id,
                                    "bbox": [x1, y1, x2, y2]
                                })
                print(f"üîç YOLO detections: {len(tracked_persons)} ng∆∞·ªùi ƒë∆∞·ª£c ph√°t hi·ªán")



                active_ids = {f"person_{p['id']}" for p in tracked_persons}

                ids_to_remove = set()

                for old_id in list(person_history.keys()):
                    last_seen = person_history[old_id][-1].get("last_seen", 0)

                    # c·∫≠p nh·∫≠t miss_count
                    if old_id not in active_ids:
                        miss_count[old_id] = miss_count.get(old_id, 0) + 1
                    else:
                        miss_count[old_id] = 0

                    # ch·ªâ x√≥a n·∫øu ng∆∞·ªùi ƒë√≥ m·∫•t > 40 frame li√™n ti·∫øp
                    if miss_count[old_id] > 40:
                        print(f"üßπ X√≥a l·ªãch s·ª≠ v√† buffer cho ID: {old_id} (m·∫•t > 40 frame)")
                        ids_to_remove.add(old_id)
                        miss_count.pop(old_id, None)
                    
                # th·ª±c hi·ªán x√≥a
                for old_id in ids_to_remove:
                    person_history.pop(old_id, None)
                    frame_buffer.pop(old_id, None)
                    kp_buffer.pop(old_id, None)
                    if "history_kps" in globals():
                        history_kps.pop(old_id, None)

                t2 = time.time()
                print(f"YOLO+DeepSORT+Cleanup: {(t2 - t1)*1000:.1f} ms")


                if not tracked_persons:
                    await ws.send_json({"predictions": {}, "ts": time.time()})
                    await asyncio.sleep(PROCESS_INTERVAL)
                    continue
                
                # Kh·ªüi t·∫°o predictions t·∫°m th·ªùi v√† g·ª≠i v·ªÅ frontend ngay (cho ph·∫£n h·ªìi nhanh)
                predictions = {}
                for d in tracked_persons:
                    pid = f"person_{d['id']}"
                    x1, y1, x2, y2 = d["bbox"]
                    pad_x = int(0.2 * (x2 - x1))
                    pad_y = int(0.2 * (y2 - y1))
                    x1 = max(0, x1 - pad_x)
                    y1 = max(0, y1 - pad_y)
                    x2 = min(frame.shape[1]-1, x2 + pad_x)
                    y2 = min(frame.shape[0]-1, y2 + pad_y)
                    if pid in person_history and len(person_history[pid]) > 0:
                        last = person_history[pid][-1]
                        predictions[pid] = {
                            "bbox": [x1, y1, x2, y2],
                            "action": last["action"],
                            "prob": last["prob"]
                        }
                    else:
                        predictions[pid] = {
                            "bbox": [x1, y1, x2, y2],
                            "action": "DETECTING",
                            "prob": 0.0
                        }
                await ws.send_json({"predictions": predictions, "ts": time.time()})


                total_t1 = time.time()

                # ================= X·ª≠ l√Ω t·∫•t c·∫£ ng∆∞·ªùi song song (OpenPose + LSTM) =================
                async def process_person(det):
                    pid = f"person_{det['id']}"
                    x1, y1, x2, y2 = det["bbox"]

                    pad_x, pad_y = int(0.2 * (x2 - x1)), int(0.2 * (y2 - y1))
                    x1 = max(0, x1 - pad_x)
                    y1 = max(0, y1 - pad_y)
                    x2 = min(frame.shape[1] - 1, x2 + pad_x)
                    y2 = min(frame.shape[0] - 1, y2 + pad_y)

                    cropped = frame[y1:y2, x1:x2]
                    if cropped.size == 0:
                        return None

                    try:
                        cropped_224 = cv2.resize(cropped, (IMG_SIZE, IMG_SIZE))
                    except Exception as e:
                        print("Resize failed:", e)
                        return None
                    
                    h = w = IMG_SIZE


                    # G·ªçi OpenPose async n·∫øu ch∆∞a c√≥ task
                    if pid not in openpose_tasks:
                        openpose_tasks[pid] = asyncio.create_task(call_openpose_async(cropped_224))

                    # L·∫•y k·∫øt qu·∫£ keypoints n·∫øu task xong
                    if openpose_tasks[pid].done():
                        res = openpose_tasks[pid].result()
                        del openpose_tasks[pid]
                        if isinstance(res, list) and len(res) > 0:
                            arr = np.array(res[0], dtype=np.float32)
                            if arr.ndim == 1 and arr.size >= 75:
                                arr = arr.reshape(-1,3)#[:,:2] 
                            # last_kps[pid] = arr.copy()
                        else:
                            arr = last_kps.get(pid, None)
                    else:
                        arr = last_kps.get(pid, None)

                    if arr is None:
                        return None

                    person_kps = arr.copy()

                    # invalid = (0,0) ho·∫∑c NaN
                    invalid_mask = (
                        ((person_kps[:,0]==0) & (person_kps[:,1]==0)) |
                        np.isnan(person_kps[:,0]) |
                        np.isnan(person_kps[:,1])
                    )

                    if pid in last_kps:
                        prev_kps = last_kps[pid]

                        valid_prev = ~(
                            ((prev_kps[:,0]==0) & (prev_kps[:,1]==0)) |
                            np.isnan(prev_kps[:,0]) |
                            np.isnan(prev_kps[:,1])
                        )

                        # 1.1. Copy t·ª´ frame tr∆∞·ªõc cho nh·ªØng ƒëi·ªÉm miss
                        fill_mask = invalid_mask & valid_prev
                        person_kps[fill_mask] = prev_kps[fill_mask]

                        # c·∫≠p nh·∫≠t l·∫°i invalid sau khi copy
                        invalid_mask = (
                            ((person_kps[:,0]==0) & (person_kps[:,1]==0)) |
                            np.isnan(person_kps[:,0]) |
                            np.isnan(person_kps[:,1])
                        )

                        # 1.2. N·ªôi suy theo h√†ng x√≥m
                        JOINT_NEIGHBORS = {
                            1: [0, 2], 2: [1, 3], 3: [2, 4],
                            5: [1, 6], 6: [5, 7], 7: [6, 8],
                            9: [8, 10], 10: [9, 11],
                            12: [11, 13], 13: [12, 14],
                            15: [0, 16], 16: [15, 17],
                            18: [17, 19], 19: [18, 20],
                            21: [0, 22], 22: [21, 23],
                            24: [23, 1],
                        }

                        for j in np.where(invalid_mask)[0]:
                            if j in JOINT_NEIGHBORS:
                                valid_refs = [r for r in JOINT_NEIGHBORS[j]
                                            if not invalid_mask[r]
                                            and not np.isnan(person_kps[r,0])
                                            and not np.isnan(person_kps[r,1])]
                                if valid_refs:
                                    person_kps[j] = np.mean(person_kps[valid_refs], axis=0)
                                    invalid_mask[j] = False

                        # c·∫≠p nh·∫≠t l·∫°i mask l·∫ßn n·ªØa
                        invalid_mask = (
                            ((person_kps[:,0]==0) & (person_kps[:,1]==0)) |
                            np.isnan(person_kps[:,0]) |
                            np.isnan(person_kps[:,1])
                        )

                        # 1.3. N·ªôi suy theo c·ª•m (block interpolation)
                        JOINT_GROUPS = {
                            "head":      [0,1,15,16,17,18],
                            "right_arm": [2,3,4],
                            "left_arm":  [5,6,7],
                            "spine":     [1,8],
                            "right_leg": [9,10,11,22,23,24],  # h√¥ng ‚Üí g·ªëi ‚Üí c·ªï ch√¢n ‚Üí b√†n ch√¢n ph·∫£i
                            "left_leg":  [12,13,14,19,20,21], # h√¥ng ‚Üí g·ªëi ‚Üí c·ªï ch√¢n ‚Üí b√†n ch√¢n tr√°i
                        }

                        prev_valid_mask = ~(
                            ((prev_kps[:,0]==0) & (prev_kps[:,1]==0)) |
                            np.isnan(prev_kps[:,0]) |
                            np.isnan(prev_kps[:,1])
                            )


                        if np.any(prev_valid_mask):
                            for group, joints in JOINT_GROUPS.items():
                                group_invalid = invalid_mask[joints]
                                group_valid = ~group_invalid


                                # N·∫øu to√†n b·ªô group m·∫•t th√¨ b·ªè qua (kh√¥ng c√≥ th√¥ng tin ƒë·ªÉ n·ªôi suy)
                                if not np.any(group_valid):
                                    continue

                                # L·∫•p v√†o c√°c kh·ªõp invalid trong c√πng group
                                for j in joints:
                                    if invalid_mask[j]:
                                        neighbors = [person_kps[k] for k in joints if not invalid_mask[k]]
                                        if len(neighbors) > 0:
                                            person_kps[j] = np.mean(neighbors, axis=0)
                                            invalid_mask[j] = False

                        invalid_mask = (
                            ((person_kps[:,0]==0) & (person_kps[:,1]==0)) |
                            np.isnan(person_kps[:,0]) |
                            np.isnan(person_kps[:,1])
                        )
                        
                        # 1.4. Qu√°n t√≠nh (prev - prevprev) cho ƒëi·ªÉm v·∫´n c√≤n miss
                        fix_mask = invalid_mask & valid_prev
                        if np.any(fix_mask):
                            prevprev = last_kps.get(f"{pid}_prevprev", prev_kps)
                            vel = prev_kps - prevprev
                            vel_norm = np.linalg.norm(vel, axis=1)

                            safe = vel_norm < 0.2 * max(w, h)
                            mask = fix_mask & safe

                            person_kps[mask] = prev_kps[mask] + vel[mask]


                        # 1.5. L√†m m∆∞·ª£t to√†n khung x∆∞∆°ng theo prev (EMA)
                        alpha = 0.3 if np.any(invalid_mask) else 0.5
                        person_kps = alpha * person_kps + (1 - alpha) * prev_kps

                    # 2. Fallback
                    invalid_mask = (
                        ((person_kps[:,0]==0) & (person_kps[:,1]==0)) |
                        np.isnan(person_kps[:,0]) |
                        np.isnan(person_kps[:,1])
                    )


                    if np.any(invalid_mask):
                        if pid in last_kps:
                            prev_kps = last_kps[pid]
                            valid_prev = ~(
                                (prev_kps[:,0]==0) | (prev_kps[:,1]==0) |
                                np.isnan(prev_kps[:,0]) | np.isnan(prev_kps[:,1])
                            )
                            if np.any(valid_prev):
                                copy_mask = invalid_mask & valid_prev
                                person_kps[copy_mask] = prev_kps[copy_mask]
                                invalid_mask = (
                                    ((person_kps[:,0]==0) & (person_kps[:,1]==0)) |
                                    np.isnan(person_kps[:,0]) | np.isnan(person_kps[:,1])
                            )

                        # N·∫øu sau khi copy t·ª´ prev m√† v·∫´n c√≤n invalid
                        if np.any(invalid_mask):
                            valid_mask = ~invalid_mask

                            # N·∫øu kh√¥ng c√≤n ƒëi·ªÉm h·ª£p l·ªá n√†o ‚Üí b·ªè frame
                            if not np.any(valid_mask):
                                return None

                            invalid_ratio = invalid_mask.sum() / person_kps.shape[0]

                            
                            # qu√° 50‚Äì60% kh·ªõp b·ªã l·ªói ‚Üí b·ªè lu√¥n
                            if invalid_ratio > 0.4:
                                return None

                            # # c√≤n l·∫°i: l·ªói √≠t ‚Üí d√πng mean ƒë·ªÉ l·∫•p
                            # mean_xy = np.mean(person_kps[valid_mask, :2], axis=0)
                            # person_kps[invalid_mask, 0] = mean_xy[0]
                            # person_kps[invalid_mask, 1] = mean_xy[1]

                    # 3. Re-center skeleton sau khi ƒë√£ fill t·∫•t c·∫£ invalid
                    # T√ôY V√ÄO ƒê·ªò ·ªîN ƒê·ªäNH C·ª¶A KHUNG X∆Ø∆†NG
                    # invalid_ratio = invalid_mask.mean()
                    # if invalid_ratio > 0.2 and pid in last_kps:
                    #     spine = person_kps[[1, 8]]
                    #     prev_spine = last_kps[pid][[1, 8]]

                    #     if not (np.any(np.isnan(spine)) or np.any(np.isnan(prev_spine))):
                    #         center = spine.mean(axis=0)
                    #         prev_center = prev_spine.mean(axis=0)
                    #         shift = np.linalg.norm(center - prev_center)

                    #         # ch·ªâ recenter khi shift kh√¥ng b·∫•t th∆∞·ªùng
                    #         if shift < 0.2 * min(w, h):  
                    #             person_kps += (prev_center - center)

                    # L∆∞u l·ªãch s·ª≠ keypoints (gi·ªØ t·ªëi ƒëa 5 frame)
                    if "history_kps" not in globals():
                        global history_kps
                        history_kps = {}

                    if pid not in history_kps:
                        history_kps[pid] = []

                    # Ch·ªâ l∆∞u keypoints h·ª£p l·ªá (kh√¥ng NaN)
                    if not np.any(np.isnan(person_kps)):
                        history_kps[pid].append(person_kps.copy())
                        if len(history_kps[pid]) > 5:  # ch·ªâ gi·ªØ 5 frame g·∫ßn nh·∫•t
                            history_kps[pid].pop(0)    

                    if pid in last_kps:
                        last_kps[f"{pid}_prevprev"] = last_kps[pid].copy()
                    last_kps[pid] = person_kps.copy()

                    if np.isnan(person_kps).any():
                        # ƒë∆∞a c√°c NaN c√≤n s√≥t v·ªÅ 0.0 (coi nh∆∞ ƒëi·ªÉm m·∫•t)
                        person_kps = np.nan_to_num(person_kps, nan=0.0)

                    # ==== DEBUG ====
                    if DEBUG:
                        debug_img = cropped_224.copy()
                        for (x, y, c) in person_kps.astype(int):
                            cv2.circle(debug_img, (x, y), 3, (0, 255, 0), -1)
                        cv2.imshow(f"debug_{pid}", debug_img)
                        cv2.waitKey(1)

                    # Normalize keypoints cho model (trong ph·∫°m vi bbox)
                    kp_norm = person_kps.copy()
                    kp_norm[:, 0] = np.clip(kp_norm[:, 0] / w, 0.0, 1.0)
                    kp_norm[:, 1] = np.clip(kp_norm[:, 1] / h, 0.0, 1.0)


                    kp_norm_xy = kp_norm[:, :2].copy().flatten()


                    if person_kps.shape[1] == 2:
                        conf_col = np.ones((person_kps.shape[0], 1), dtype=np.float32)
                        person_kps = np.concatenate([person_kps, conf_col], axis=1)

                    # Flatten keypoints v√† l∆∞u buffer
                    keypoints_flat = kp_norm.flatten()

                    last_kps[pid] = person_kps.copy()

                    # n·∫øu thi·∫øu keypoints
                    if keypoints_flat.size < NUM_FEATURES:
                        keypoints_flat = np.pad(keypoints_flat, (0, NUM_FEATURES - keypoints_flat.size))
                    elif keypoints_flat.size > NUM_FEATURES:
                        keypoints_flat = keypoints_flat[:NUM_FEATURES]
                        
                    frame_buffer.setdefault(pid, []).append(cropped_224)

                    # ki·ªÉm tra shape cho ch·∫Øc
                    if cropped_224.shape != (IMG_SIZE, IMG_SIZE, 3):
                        print("Skipping frame due to wrong shape:", cropped_224.shape)
                        return None


                    kp_buffer.setdefault(pid, []).append(keypoints_flat)
                    if len(frame_buffer[pid]) > NUM_FRAMES:
                        frame_buffer[pid].pop(0)
                        kp_buffer[pid].pop(0)


                    #####DEBUG#####
                    # print(f"len(keypoints_flat)={len(keypoints_flat)}, has_nan={np.isnan(keypoints_flat).any()}")
                    # print(f"bbox=({x1},{y1},{x2},{y2}), crop size={cropped.shape}, kp_minmax=({person_kps[:,0].min():.1f},{person_kps[:,0].max():.1f})")
                    # print(f"[DEBUG] PID={pid}: crop_shape={cropped.shape}, kps_minmax=({arr[:,0].min():.1f},{arr[:,0].max():.1f}), ({arr[:,1].min():.1f},{arr[:,1].max():.1f})")
                    
                        
                    # ===== D·ª± ƒëo√°n v·ªõi LSTM =====
                    if len(frame_buffer[pid]) >= NUM_FRAMES:
                        # l·∫•y ƒë√∫ng 12 frame
                        frames_list = frame_buffer[pid][-NUM_FRAMES:]
                        kps_list    = kp_buffer[pid][-NUM_FRAMES:]

                        # l·ªçc l·∫°i cho ch·∫Øc (ƒë·ªÅ ph√≤ng frame l·ªói shape / kp thi·∫øu)
                        frames_list = [f for f in frames_list if f.shape == (IMG_SIZE, IMG_SIZE, 3)]
                        kps_list    = [k for k in kps_list if k.size == NUM_FEATURES]

                        # n·∫øu c√≤n < NUM_FRAMES th√¨ b·ªè
                        if len(frames_list) < NUM_FRAMES or len(kps_list) < NUM_FRAMES:
                            pass
                        else:
                            action, prob = await do_model_predict(frames_list, kps_list)
                            if action != "ERROR" and prob > 0.65:
                                entry = {
                                    "time": time.time(),
                                    "bbox": [x1, y1, x2, y2],
                                    "action": action,
                                    "prob": round(prob, 3),
                                    "keypoints": kp_norm_xy.tolist(),
                                    "last_seen": time.time()
                                }
                                person_history.setdefault(pid, []).append(entry)
                                if len(person_history[pid]) > 200:
                                    person_history[pid] = person_history[pid][-200:]
                                return {
                                    "pid": pid,
                                    "bbox": [x1, y1, x2, y2],
                                    "action": action,
                                    "prob": round(prob, 3),
                                    "time": time.strftime("%H:%M:%S", time.localtime()),
                                    "keypoints": kp_norm_xy.tolist()
                                }

                    # N·∫øu ch∆∞a ƒë·ªß frame ho·∫∑c ch∆∞a predict m·ªõi, d√πng action cu·ªëi c√πng n·∫øu c√≥
                    if pid in person_history and len(person_history[pid]) > 0:
                        last = person_history[pid][-1]
                        return {
                            "pid": pid,
                            "bbox": [x1, y1, x2, y2],
                            "action": last["action"],
                            "prob": last["prob"],
                            "time": time.strftime("%H:%M:%S", time.localtime()),
                            "keypoints": kp_norm_xy.tolist(),
                        }

                    return None


                # ch·∫°y t·∫•t c·∫£ ng∆∞·ªùi song song 
                person_tasks = [asyncio.create_task(process_person(det)) for det in tracked_persons]
                results = await asyncio.gather(*person_tasks)

                predictions = {r["pid"]: {"bbox": r["bbox"], "action": r["action"], "prob": r["prob"], "keypoints": r.get("keypoints", None)} for r in results if r}
                
                total_t2 = time.time()
                print(f"Multi-person total time: {(total_t2 - total_t1)*1000:.1f} ms")

                # G·ª≠i k·∫øt qu·∫£ cu·ªëi c√πng (c·∫≠p nh·∫≠t n·∫øu c√≥ d·ª± ƒëo√°n m·ªõi t·ª´ LSTM)
                try:
                    if predictions:
                        await ws.send_json({"predictions": predictions, "ts": time.time()})
 
                except Exception as e:
                    print("send fail:", e)
                    return

                await asyncio.sleep(PROCESS_INTERVAL)

        except WebSocketDisconnect:
            return
        except asyncio.CancelledError:
            return
        except Exception as e:
            print("Processor unexpected error:", e)
            return


    recv_task = asyncio.create_task(receiver())
    proc_task = asyncio.create_task(processor())

    done, pending = await asyncio.wait([recv_task, proc_task], return_when=asyncio.FIRST_COMPLETED)
    for p in pending:
        p.cancel()

    print("Client disconnected")
    try:
        await ws.close()
    except Exception:
        pass
    
# ================== ALERT FALL API ==================
@app.post("/alert-fall")
async def alert_fall(
    background_tasks: BackgroundTasks,
    image: UploadFile = File(...),
    message: str = Form("Ph√°t hi·ªán h√†nh ƒë·ªông nguy hi·ªÉm t·ª´ h·ªá th·ªëng!")
):
    """
    Nh·∫≠n ·∫£nh + message t·ª´ frontend v√† g·ª≠i sang Telegram ·ªü background,
    tr·∫£ response cho frontend NGAY L·∫¨P T·ª®C, kh√¥ng block event loop.
    """
    try:
        file_bytes = await image.read()

        url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendPhoto"

        files = {
            "photo": ("fall_frame.jpg", file_bytes, image.content_type)
        }
        data = {
            "chat_id": TELEGRAM_CHAT_ID,
            "caption": message
        }

        def send_telegram():
            try:
                resp = requests.post(url, data=data, files=files, timeout=10)
                if resp.status_code != 200:
                    print("Telegram response:", resp.text)
                else:
                    print("Sent alert to Telegram")
            except Exception as e:
                print("Telegram send error:", e)

        # ch·∫°y g·ª≠i Telegram ·ªü background, kh√¥ng ch·∫∑n request ch√≠nh
        background_tasks.add_task(send_telegram)

        # Tr·∫£ v·ªÅ ngay cho frontend
        return {"ok": True}
    except Exception as e:
        print("alert_fall error:", e)
        return {"ok": False, "error": str(e)}

    
if __name__ == "__main__":
    uvicorn.run(app, host="127.0.0.1", port=8000)



